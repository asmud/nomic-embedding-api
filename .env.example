# =================================
# Nomic Embedding API Configuration
# =================================

# Copy this file to .env and modify as needed
# Example: cp .env.example .env

# =================================
# SERVER CONFIGURATION
# =================================

# Server host (default: 0.0.0.0 - all interfaces)
# Use 127.0.0.1 for localhost only
HOST=0.0.0.0

# Server port (default: 8000)
PORT=8000

# =================================
# MODEL CONFIGURATION (SIMPLIFIED)
# =================================

# Choose your embedding model preset:
# 
# EMBEDDING_MODEL=nomic-768   -> 768 dimensions, high quality (default)
#   Model: nomic-ai/nomic-embed-text-v2-moe
#   Library: Transformers
#   Use case: High accuracy applications
#
# EMBEDDING_MODEL=nomic-256   -> 256 dimensions, fast inference
#   Model: Abdelkareem/nomic-embed-text-v2-moe_distilled  
#   Library: Model2Vec
#   Use case: Speed-optimized applications
#
EMBEDDING_MODEL=nomic-768

# =================================
# CACHING CONFIGURATION
# =================================

# HuggingFace cache directory (default: ./models)
# This is where models will be downloaded and stored
HF_HOME=./models

# Transformers specific cache directory
# If not set, falls back to HF_HOME
TRANSFORMERS_CACHE=./models

# Sentence Transformers cache directory
# If not set, falls back to HF_HOME
SENTENCE_TRANSFORMERS_HOME=./models

# =================================
# PERFORMANCE CONFIGURATION
# =================================

# CUDA/GPU Configuration
# Set to "cpu" to force CPU-only inference
# Leave empty for automatic GPU detection
# CUDA_VISIBLE_DEVICES=0

# PyTorch settings for memory optimization
# PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# =================================
# LOGGING CONFIGURATION
# =================================

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# Enable detailed model loading logs
# VERBOSE_LOADING=true

# =================================
# API CONFIGURATION
# =================================

# Enable CORS for cross-origin requests
# ENABLE_CORS=true

# API request timeout in seconds
# REQUEST_TIMEOUT=300

# Maximum batch size for embeddings
# MAX_BATCH_SIZE=100

# =================================
# PERFORMANCE SETTINGS
# =================================

# Maximum batch size for processing multiple requests together
MAX_BATCH_SIZE=32

# Timeout in milliseconds to wait for more requests to batch
BATCH_TIMEOUT_MS=50

# Maximum number of embeddings to cache in memory
CACHE_SIZE=1000

# Enable/disable in-memory caching
ENABLE_CACHING=true

# Enable quantization (float16) for better performance on GPU
ENABLE_QUANTIZATION=true

# Enable torch.compile for PyTorch 2.0+ optimization (experimental)
TORCH_COMPILE=false

# Maximum number of concurrent requests to process
MAX_CONCURRENT_REQUESTS=100

# =================================
# MODEL POOL CONFIGURATION
# =================================

# Model pool size (0 = disabled, auto-detects optimal size)
# When enabled, creates multiple model instances for better concurrency
MODEL_POOL_SIZE=0

# Enable multi-GPU support for model pool
ENABLE_MULTI_GPU=true

# Health check interval for model pool (seconds)
HEALTH_CHECK_INTERVAL=30

# Maximum error count before marking model instance as failed
MAX_MODEL_ERROR_COUNT=5

# =================================
# REDIS CONFIGURATION (OPTIONAL)
# =================================

# Enable Redis for distributed caching and session management
# Set to "true" to enable Redis support
REDIS_ENABLED=false

# Redis connection URL
# Examples:
# - Docker/Container: redis://redis:6379 (recommended for Docker deployments)  
# - Local Redis: redis://localhost:6379 (for local development)
# - Redis with auth: redis://:password@redis:6379
# - Redis Cluster: redis://redis1:6379,redis2:6379,redis3:6379
REDIS_URL=redis://redis:6379

# Redis database number (0-15)
REDIS_DB=0

# Redis key prefix for this application
REDIS_KEY_PREFIX=nomic_embedding

# Redis connection pool settings
REDIS_MAX_CONNECTIONS=20
REDIS_RETRY_ON_TIMEOUT=true

# Redis cache TTL (time to live) in seconds
# Default: 3600 (1 hour)
REDIS_CACHE_TTL=3600

# Redis session TTL for user rate limiting (seconds)
# Default: 300 (5 minutes)
REDIS_SESSION_TTL=300

# =================================
# MEMORY MANAGEMENT (OPTIONAL)
# =================================

# Enable dynamic memory management
# When enabled, cache and batch sizes auto-adjust based on memory pressure
ENABLE_DYNAMIC_MEMORY=true

# Memory monitoring interval in seconds
# How often to check memory usage and adjust settings
MEMORY_MONITORING_INTERVAL=30.0

# Memory pressure thresholds (percentage of total memory)
# High pressure threshold - start reducing cache/batch sizes
MEMORY_PRESSURE_HIGH_PERCENT=85.0

# Critical pressure threshold - aggressive reduction
MEMORY_PRESSURE_CRITICAL_PERCENT=95.0

# Minimum available memory in MB before taking action
MIN_AVAILABLE_MEMORY_MB=512

# Enable garbage collection optimization
# Automatically tune GC thresholds based on memory patterns
ENABLE_GC_OPTIMIZATION=true

# =================================
# HARDWARE OPTIMIZATION (OPTIONAL)
# =================================

# Enable comprehensive hardware optimization
# Includes auto device selection, workload profiling, and dynamic scaling
ENABLE_HARDWARE_OPTIMIZATION=true

# Optimization aggressiveness level
# conservative: Safe settings, minimal performance impact
# balanced: Good performance with stability (default)
# aggressive: Maximum performance, may use more resources
OPTIMIZATION_LEVEL=balanced

# Enable CPU affinity for performance (Linux only)
# Pins processes to specific CPU cores
ENABLE_CPU_AFFINITY=false

# Enable NUMA (Non-Uniform Memory Access) awareness
# Optimizes memory allocation for multi-socket systems
ENABLE_NUMA_AWARENESS=false

# Enable automatic device selection
# Automatically chooses optimal GPU/CPU for inference
AUTO_DEVICE_SELECTION=true

# Enable PyTorch optimizations
# Includes thread tuning, CUDA optimizations, etc.
ENABLE_TORCH_OPTIMIZATION=true

# =================================
# PRODUCTION SERVER SETTINGS
# =================================

# Production environment marker
# Values: development, production, testing
ENVIRONMENT=production

# Server mode configuration
# Set to "true" to force gunicorn multi-worker mode
USE_GUNICORN=false

# Gunicorn worker configuration
# "auto" = CPU count, or specify number (e.g., "4")
GUNICORN_WORKERS=auto

# Gunicorn timeout settings (seconds)
GUNICORN_TIMEOUT=300
GUNICORN_KEEPALIVE=2

# Worker restart settings
GUNICORN_MAX_REQUESTS=10000
GUNICORN_MAX_REQUESTS_JITTER=1000

# Uvicorn single-process settings (when USE_GUNICORN=false)
# Maximum concurrent connections
UVICORN_LIMIT_CONCURRENCY=1000

# Maximum requests before restart
UVICORN_LIMIT_MAX_REQUESTS=10000

# Keep-alive timeout in seconds
UVICORN_TIMEOUT_KEEP_ALIVE=5

# Graceful shutdown timeout in seconds
UVICORN_TIMEOUT_GRACEFUL_SHUTDOWN=30

# Maximum size of incomplete HTTP events
H11_MAX_INCOMPLETE_EVENT_SIZE=16384

# SSL/TLS Configuration (optional)
# SSL_KEYFILE=/path/to/private.key
# SSL_CERTFILE=/path/to/certificate.crt
# SSL_CA_CERTS=/path/to/ca-bundle.crt

# =================================
# ADVANCED SETTINGS
# =================================

# Trust remote code when loading models (required for some models)
TRUST_REMOTE_CODE=true

# Enable tokenizer parallelism for better performance
TOKENIZERS_PARALLELISM=true

# Torch dtype for model inference
# Options: float32, float16, bfloat16
# TORCH_DTYPE=float16

# Use memory mapping for faster model loading
# USE_MMAP=true